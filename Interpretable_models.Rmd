---
title: "Interpretable models"
author: "Jérémy Feteira"
date: ""
lang: fr
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
  pdf_document:
    df_print: kable
    keep_tex: yes
    number_section: yes
    toc: yes
  html_document:
    df_print: paged
    code_folding: show
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10, sanitize = TRUE
  )
```

```{r packages}
library(readxl)
library(stargazer)
library(readr)
#library(kableExtra)
```

```{r function}
# si above = TRUE on a un header_above
tab_fun <- function(tab, above = FALSE, title = title, font_size = 10, header = NULL){
  if(above){
    tab %>% kable(caption = title) %>%
    kable_styling(font_size = font_size, full_width=FALSE, stripe_color = "lightgray", stripe_index = 0,
                  latex_options = c("HOLD_position", "striped"), position = "center") %>%
    add_header_above(header = header, bold=TRUE, color="red")%>%
    column_spec(1, bold=T) %>%
    row_spec(0, bold=T)
  } else {
    tab %>% kable(caption = title) %>%
      kable_styling(font_size = font_size, full_width=FALSE, stripe_color = "lightgray", stripe_index = 0,
                    latex_options = c("HOLD_position", "striped"), position = "center") %>%
      column_spec(1, bold=T) %>%
      row_spec(0, bold=T)
  }
}
```


# Importation of the databases

```{r}
setwd("D:/Memoire/application")
```

```{r}
data_regression <- read_excel("data_regression.xlsx")
data_classification <- read_excel("data_classification.xlsx")
```

```{r}
table(is.na(data_classification))
table(is.na(data_regression))
```


# Linear models

This part is for the linear models I created for my research paper.

## Linear regression

I created the following model: 

$ln(price_i) = \alpha_i + \beta_{1,i} mileage_i+ \beta_{2,i} gear_i + \beta_{3,i} year_i + \beta_{4,i} offerType_i + \epsilon_i$. 

The objective is to find what are the features that have an impact on the price of the car. I did not put the `hp` feature because it has a high correlation with prices (0.75) so we already know that the horsepower of a car has an impact on the price.

```{r}
lm_regression <- lm(log(price)~mileage+gear+year+offerType+mileage:gear, data=data_regression)
```

Now we do the Breusch-Pagan test in order to look at the heteroskedasticity:

```{r}
library(lmtest)
bptest(lm_regression)
```

There is heteroskedasticity in the model.

```{r}
data_regression_bis <- data_regression
data_regression_bis$residuals_stringency <- lm_regression$residuals
varfunc.ols2 <- lm(
  log(residuals_stringency^2) ~ mileage + gear + year + offerType + mileage:gear,
  data=data_regression_bis
  )

data_regression_bis$varfunc2 <- exp(varfunc.ols2$model[1,1])

lm_regression_bis <- lm(
  log(price)~mileage+gear+year+offerType+mileage:gear, 
  data=data_regression_bis, weights=1/sqrt(varfunc2)
  )
```

I have removed heteroskedasticity. The model is now a WLS (weight least squares) model and not OLS anymore because I put weights on the values to correct the heteroscedasticity. The stadars errors are now robusts. We can then look at the weights of the model:

```{r}
summary(lm_regression_bis)
#stargazer(lm_regression_bis, type="latex")
```

mileage*gearManual: 
"when the car has a manual gear, we observe an increase of `r 100*(exp(0.0015)-1)`% in prices for cars with 1000 kms in comparison to a car that has automatic gear".


## Logistic regression

Here, I calculated the probability that a car has an automatic or manual gear. For this example, I removed the observations with semi-automatic gear.

```{r}
data_regression_logit <- data_regression[data_regression$gear!="Semi-automatic",]
levels(as.factor(data_regression_logit$gear))
data_regression_logit$gear <- as.factor(data_regression_logit$gear)
```

```{r}
logit_regression <- glm(gear~mileage+fuel+hp+year+offerType, data=data_regression_logit,
                        family="binomial")
summary(logit_regression)
#stargazer(logit_regression, type="latex")
```

If the fuel is electricity and gasoline, the estimated odds change by a factor of `r exp(-1.875)` in comparison to CNG (compressed natural gas). This means that there is not much chance to find a car with manual gear and with fuel electricity and gasoline.

```{r}
exp(cbind(OR = coef(logit_regression), confint(logit_regression)))
```

def odds-ratio: "the odds ratio is the probability of the event divided by the probability of the nonevent" (http://www.appstate.edu/~whiteheadjc/service/logit/intro.htm) (https://stats.idre.ucla.edu/r/dae/logit-regression/)

```{r}
nrow(data_regression_logit[data_regression_logit$gear=="Manual"&data_regression_logit$fuel=="Electric/Gasoline",]) / nrow(data_regression_logit[data_regression_logit$gear=="Automatic"&data_regression_logit$fuel=="Electric/Gasoline",])
## divided by
nrow(data_regression_logit[data_regression_logit$gear=="Manual"&data_regression_logit$fuel!="Electric/Gasoline",]) / nrow(data_regression_logit[data_regression_logit$gear=="Automatic"&data_regression_logit$fuel!="Electric/Gasoline",])

table(data_regression_logit$gear, data_regression_logit$fuel)
```

```{r}
logit_regression <- glm(gear~fuel, data=data_regression_logit, family="binomial")
summary(logit_regression)
#stargazer(logit_regression, type="latex")
```

If there is just the intercept in the model, then if we get the exponential of the weight, we get the frequency of the outcome when it is not the category of reference.

When only one binary variable: if the exponential of the weight is 1.50, then the odds for the category are 50% higher than the odds for the reference category.

When only one continuous variable: we calculate, using the weights of the intercept and the variable, by using a number for the variable. For example, for the mileage value we could use 100 000 kms and compare it to 110 000 kms. Then, we make the difference of the 2 values and we get the exponential of the result. It gives us the odds. The explanation is the same as before and it does not depend on the values we chose (here 100 000 kms and 110 000 kms). In this example we have a 10 000 kms increase.

When several variables and no interaction: With Y being the outcome, holding the other variables at a fixed value, the odds of Y=1 for the other category of the variable over the odds of Y=1 for the reference category is the exponential of the weight.

When there are interactions: For example if we put female and math in interaction, we get the following interpretation: "for the female students, a one-unit increase in math score yields a change in log odds of" $weight_{math} + weight_{mathXfemale}$

https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/

## GLM and GAM

### GLM

An example for GLM is the logit model which uses a binomial distribution for the outcome feature. Another example is a Poisson regression. The outcome follows a Poisson distribution. For this example, I took the model and the database of this website: https://stats.idre.ucla.edu/r/dae/poisson-regression/. The Poisson distribution is used for count features as outcome. Count data are features that consist of discrete features that are non-negative integers.

```{r}
data_glm_poisson <- read.csv("poisson_sim.csv")
mean(data_glm_poisson$num_awards)
var(data_glm_poisson$num_awards)
```

Because the variance and the mean are close and that the range is between 0 and 6, we can consider that we can use the Poisson distribution to create our model (https://www.theanalysisfactor.com/count-vs-continuous-variables-differences-under-the-hood/).

```{r}
data_glm_poisson <- within(
  data_glm_poisson, {
    prog <- factor(prog, levels=1:3, labels=c("General", "Academic", "Vocational"))
    id <- factor(id)
    }
  )

with(data_glm_poisson, tapply(
  num_awards, prog, function(x) {sprintf("Mean (Variance) = %1.2f (%1.2f)", mean(x), var(x))}
  )
  )
```

Moreover, the mean and conditional variance are also close for each level of the feature prog. The model has for explanatory variables prog and math and for outcome num_awards.

```{r}
glm_Poisson <- glm(num_awards~prog+math, family="poisson", data=data_glm_poisson)
bptest(glm_Poisson)
```

The Breusch-Pagan test tells us that there is heteroscedasticity in the model.

```{r}
data_glm_poisson$residuals_stringency <- glm_Poisson$residuals
varfunc.ols2 <- glm(residuals_stringency^2~prog+log(math), family="poisson", data=data_glm_poisson)

data_glm_poisson$varfunc2 <- exp(varfunc.ols2$model[1,1])

glm_Poisson <- glm(
  num_awards~prog+math, data=data_glm_poisson, family="poisson", weights=1/sqrt(varfunc2)
  )
```

In order to test if there is over-dispersion, we can use the following package and function.

```{r}
library(AER)
dispersiontest(glm_Poisson,trafo=1)
```

Because the p-value is greater than 0.05, we can confirm that there are no over-dispersion.

```{r}
summary(glm_Poisson)
#stargazer(glm_Poisson, type="latex")
```

- "The expected log count for a one-unit increase in math is 0.07"

- "The expected log count for 'Academic' increases by about 1.1" in comparison to "General"

To test the model, we can use the residual deviance which is the difference between the deviance of the best model and the deviance of the model created:

```{r}
tab_fun(
  with(glm_Poisson, 
  cbind(res.deviance = deviance, df = df.residual, p = pchisq(deviance, df.residual, lower.tail=FALSE))
  ), title="Residual deviance of the model")
```



### GAM

For this example, I did a GAM model on the dataset of the German cars. The outcome is the mileage and the explanatory features are the price and the horsepower (hp).

```{r}
library(mgcv)
gam_regression <- gam(mileage~s(price)+s(hp), data=data_regression)

summary(gam_regression)
#stargazer(gam_regression, type="latex")
```

(https://m-clark.github.io/generalized-additive-models/application.html#single-predictor)

First of all, every weights are significantly different than 0 and the adjusted R square is 0.591. The term "edf" is the "effective degrees of freedom". The GCV (generalized cross validation) score is "an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process". It estimates the model by removing an observation and then it notes the "squared residual predicting observation $i$ from the model" and do the same thing for every other observation. It can help to choose the best model between several models and as for AIC, the lower the better.

# Decision tree

# Decision rules

## OneR

## RIPPER

## BRL

# RuleFit

## K-nearest neighbors









