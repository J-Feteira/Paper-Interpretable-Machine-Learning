---
title: "What are the local methods that allow us to understand an individual prediction of a black box model?"
author: "Jérémy Feteira"
date: ""
lang: fr
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
  pdf_document:
    df_print: kable
    keep_tex: yes
    number_section: yes
    toc: yes
  html_document:
    df_print: paged
    code_folding: show
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10, sanitize = TRUE, cache=TRUE
  )
```

```{r package}
library(doParallel)
library(kableExtra)
library(ggplot2)
library(randomForest)
```



# Importation of the databases and random forests

```{r}
tryCatch(
  expr={source("D:/Memoire/rendu_final_code/import.R")},
  error=function(e){source("C:/Perso/Memoire/application/import.R")}
)
```

```{r}
table(is.na(data_classification))
table(is.na(data_regression))
```



# Local surrogate (LIME)

## Iml package

http://uc-r.github.io/iml-pkg

### German cars dataset

```{r}
require(iml)
```

```{r}
pred_fun <- function(X.model, newdata) {predict(X.model, newdata)}

predictor <- Predictor$new(
  rf_optimal_regression_bis, data=train_regression[,-5], y=train_regression$price,
  predict.function=pred_fun
  )
```


```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

LIME_model_1 <- LocalModel$new(predictor, x.interest=train_regression[1,-c(5)], k=5)
LIME_model_2 <- LocalModel$new(predictor, x.interest=train_regression[35,-c(5)], k=5)

stopImplicitCluster()
```

```{r}
plot_LIME_1 <- plot(LIME_model_1) + ylab("Effect") + xlab("Features") + aes(fill="darkred") +
  scale_fill_manual(values="darkred") + ylim(-15000, 20000) +
  ggtitle("") +
  theme_classic() + theme(legend.position="none", text=element_text(size=22)) 

plot_LIME_2 <- plot(LIME_model_2) + ylab("Effect") + ggpubr::rremove("xlab") + aes(fill="darkred") +
  scale_fill_manual(values="darkred") + ylim(-15000, 20000) +
  ggtitle("") +
  theme_classic() + theme(legend.position="none", text=element_text(size=22))

ggpubr::ggarrange(plot_LIME_1, plot_LIME_2) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Lime of the observation number 1 and 35 for the random forest on the German cars dataset",
      size=22
      )
  )
```

For the first observation, it seems that the automatic gear and the fuel diesel have a positive impact on the predicted price. On the other hand, the year 2014, the fact that the car is used and have 130,000 kms have a negative impact on the predicted price.

For the 35th observation, the automatic gear and the year 2020 have a positive impact on the predicted price. Mileage seems to have no impact on the predicted price because it is too low and the gasoline fuel and the fact that the car has been used, decrease the predicted price.

From these two observations, it appears that the automatic gear, the diesel and a recent year increase the predicted price. On the other hand, the fact that the car has been used, the high mileage and a recent year seems to decrease the predicted price in our model.


### Marks in Portugal dataset

```{r}
require(iml)
```

```{r}
pred_fun_bis <- function(X.model, newdata) {predict(X.model, newdata, type="prob")}

predictor_classification <- Predictor$new(
  rf_best_classification_bis, data=train[,-c(32)], y=train$average_mark_factor,
  predict.function=pred_fun_bis
  )
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

LIME_model_1_class <- LocalModel$new(predictor_classification, x.interest=train[1,-c(32)], k=5)
LIME_model_2_class <- LocalModel$new(predictor_classification, x.interest=train[35,-c(32)], k=5)

stopImplicitCluster()
```

```{r}
plot_LIME_1_class <- plot(LIME_model_1_class) + ylab("Effect") + xlab("Features") +
  aes(fill="darkred") + scale_fill_manual(values="darkred") + ylim(-1, 1) +
  ggtitle("") + theme_classic() + theme(legend.position="none", text=element_text(size=22)) 

plot_LIME_2_class <- plot(LIME_model_2_class) + ylab("Effect") + ggpubr::rremove("xlab") +
  aes(fill="darkred") + scale_fill_manual(values="darkred") + ylim(-1, 1) +
  ggtitle("") + theme_classic() + theme(legend.position="none", text=element_text(size=22))

ggpubr::ggarrange(plot_LIME_1_class, plot_LIME_2_class) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Lime of the observation number 1 and 35 for the random forest on the marks in Portugal dataset",
      size=22
      )
  )
```

For the observation number 1, the fact that is has 1 failure increase the predicted mark of between 0 and 10. On the other hand, the fact that the student wants to do higher education increase the prediction of a mark between 10 and 20. For the observation number 35, the fact that he wants to do higher education and that the student has no failure increase the prediction of a mark between 10 and 20.


## Lime package

For this part I used the lime package (https://uc-r.github.io/lime). This package uses the true LIME algorithm which is not the case for the LocalModel function of the iml package but I had more difficulties to use the LIME package due to the fact that the function explain allows XGBoost or caret models but not randomForest models.

### German cars dataset

```{r}
library(lime)
load("D:/Memoire/application/best_model_random_forest_regression.RData")
```

```{r}
explainer_rf <- lime(train_regression, rf_optimal_regression, n_bins=2) ## bins to default=4
```

Then we need to choose some values that will be used for the local interpretation. I decided to take the same as before, the number 1 and 35.

```{r}
local_obs <- train_regression[c(1,35),]
```

Now we can use the function explain to explain the model predictions.

```{r}
explanation_rf <- lime::explain(
  x=local_obs, explainer=explainer_rf, n_features=6, dist_fun="gower",
  n_permutations=5000, feature_select="forward_selection"
  )
```

The method used here is the "forward_selection" which is when the method "adds one feature at a time until n_features is reached, based on quality of a ridge regression model".

```{r}
plot_features(explanation_rf) + theme_classic() +
  ggtitle("LIME for observation number 1 and 35 for the random forest on the German cars dataset") +
  theme(legend.position="none", text=element_text(size=22))
```


### Marks in Portugal dataset

```{r}
load("D:/Memoire/application/best_model_random_forest_classification.RData")
```

```{r}
explainer_rf_class <- lime(train, rf_best_classification, n_bins=2) ## bins to default=4
```

```{r}
local_obs_class <- train[c(1,35),]
```

```{r}
explanation_rf_class <- explain(
  x=local_obs_class, explainer=explainer_rf_class, n_features=10, dist_fun="gower",
  n_permutations=5000, feature_select="auto", labels="mark [0,10]"
  )
```

```{r}
plot_features(explanation_rf_class) + theme_classic() +
  ggtitle("LIME for observation number 1 and 35 for the random forest on the marks in Portugal dataset") +
  theme(legend.position="none", text=element_text(size=22))
```


# Shapley values

## Iml package

### German cars dataset

The iml package is useful to look at the contributions of the categories: http://uc-r.github.io/iml-pkg and http://xai-tools.drwhy.ai/iml.html#predict_parts.

```{r}
require(iml)
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

shapley_rf_1 <- Shapley$new(predictor, x.interest=train_regression[1,-c(5)])
shapley_rf_2 <- Shapley$new(predictor, x.interest=train_regression[35,-c(5)])

stopImplicitCluster()
```

```{r}
plot_shapley_1 <- plot(shapley_rf_1) + ylab("Shapley value") + aes(fill="darkred") +
  scale_fill_manual(values="darkred") + ggtitle("") + ylim(-50000, 50000) +
  theme_classic() + theme(legend.position="none", text=element_text(size=22))

plot_shapley_2 <- plot(shapley_rf_2) + ylab("Shapley value") + aes(fill="darkred") +
  scale_fill_manual(values="darkred") + ggtitle("") + ylim(-50000, 50000) +
  theme_classic() + theme(legend.position="none", text=element_text(size=22))

ggpubr::ggarrange(plot_shapley_1, plot_shapley_2) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Shapley values for the observations number 1 and 35 on the German cars dataset",
      size=22
      )
  )
```

First of all, the average prediction for this method on the random forest done is $16,576.12$ whereas the actual prediction of the Shapley value of the first observation is only of $15,921.38$ which gives a difference of $-654.74$. This means that the actual predicted price of this instance is below the average predicted price. In comparison, the actual prediction is of $29,892.7$ for the Shapley value of the observation number 35 which gives a difference of $13,316.58$. In this case, the actual predicted price is $29,892.7$ which is greater than the average predicted price of $16,576.12$.

For the first observation, the gear being automatic and the 2014 year has a positive contribution on the prediction. On the other hand, the mileage of 130,000 kms has a negative contribution on the predicted price.

For the observation number 35, the mileage of 5 kms and the automatic gear have a positive contribution on the predicted price. On the other hand, the 2020 year has a negative impact on the predicted price.


### Marks in Portugal dataset

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

shapley_rf_1_class <- Shapley$new(predictor_classification, x.interest=train[1,-c(32)])
shapley_rf_2_class <- Shapley$new(predictor_classification, x.interest=train[35,-c(32)])

stopImplicitCluster()
```

```{r}
plot_shapley_1_class <- plot(shapley_rf_1_class) + ylab("Shapley value") + aes(fill="darkred") +
  scale_fill_manual(values="darkred") + ggtitle("") + ylim(-0.5, 0.5) +
  theme_classic() + theme(legend.position="none", text=element_text(size=22))

plot_shapley_2_class <- plot(shapley_rf_2_class) + ylab("Shapley value") + aes(fill="darkred") +
  scale_fill_manual(values="darkred") + ggtitle("") + ylim(-0.5, 0.5) +
  theme_classic() + theme(legend.position="none", text=element_text(size=22))

ggpubr::ggarrange(plot_shapley_1_class, plot_shapley_2_class) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Shapley values for the observations number 1 and 35 on the marks in Portugal dataset",
      size=22
      )
  )
```

From what we can see it looks like that the failure has the highest impact on the prediction for both observation. For the observation number 1, the fact that the student has one failure, increase the probability of having a bad mark the most. On the other hand, the fact that the student number 35 has 0 failure increase the probability of having a good mark the most.


## Fastshap package

The fastshap package is used for more global interpretation because we look at the features and not categories and the Shapley values are approximations (https://bgreenwell.github.io/fastshap/articles/fastshap.html).

```{r}
library(fastshap)
```

```{r}
pred_fun <- function(X.model, newdata) {predict(X.model, newdata)}
```


```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

shap_1 <- fastshap::explain(
  rf_optimal_regression_bis, X=as.data.frame(train_regression), pred_wrapper=pred_fun, nsim=10,
  feature_names=c("mileage", "fuel", "gear", "offerType", "year"),
  newdata=as.data.frame(train_regression[1,])
  )

shap_2 <- fastshap::explain(
  rf_optimal_regression_bis, X=as.data.frame(train_regression), pred_wrapper=pred_fun, nsim=10,
  feature_names=c("mileage", "fuel", "gear", "offerType", "year"),
  newdata=as.data.frame(train_regression[35,])
  )

stopImplicitCluster()
```

```{r}
# Aggregate Shapley values
shap_imp_1 <- data.frame(
  Variable=names(shap_1), Importance=apply(shap_1, MARGIN=2, FUN = function(x) sum(abs(x)))
)
shap_imp_2 <- data.frame(
  Variable=names(shap_2), Importance=apply(shap_2, MARGIN=2, FUN = function(x) sum(abs(x)))
)
```


```{r}
# Plot Shap-based variable importance
plot_shapley_1_bis <- ggplot(shap_imp_1, aes(reorder(Variable, Importance), Importance)) +
  aes(fill="darkred") + scale_fill_manual(values="darkred") +
  geom_col(fill="darkred") + xlab("Features") + ylab("Shapley value") + ylim(0, 85000) +
  ggtitle("") + theme_classic() + theme(legend.position="none", text=element_text(size=22))

plot_shapley_2_bis <- ggplot(shap_imp_2, aes(reorder(Variable, Importance), Importance)) +
  aes(fill="darkred") + scale_fill_manual(values="darkred") +
  geom_col(fill="darkred") + xlab("Features") + ylab("Shapley value") + ylim(0, 85000) +
  ggtitle("") + theme_classic() + theme(legend.position="none", text=element_text(size=22))

ggpubr::ggarrange(plot_shapley_1_bis, plot_shapley_2_bis) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Shapley values for the observations number 1 and 35 on the German cars dataset",
      size=22
      )
  )
```

For the observation number 1, the gear, the year and the mileage contribute the most in the predicted price. On the other plot, for the observation number 35, the mileage and the year are contributing the most.

```{r}
pred_fun_bis <- function(X.model, newdata) {predict(X.model, newdata, type="prob")}
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

shap_1 <- fastshap::explain(
  rf_best_classification_bis, X=as.data.frame(train), pred_wrapper=pred_fun_bis, nsim=10,
  newdata=as.data.frame(train[1,])
  )

shap_2 <- fastshap::explain(
  rf_best_classification_bis, X=as.data.frame(train), pred_wrapper=pred_fun_bis, nsim=10,
  newdata=as.data.frame(train[35,])
  )

stopImplicitCluster()
```

```{r}
# Aggregate Shapley values
shap_imp_1 <- data.frame(
  Variable=names(shap_1), Importance=apply(shap_1, MARGIN=2, FUN = function(x) sum(abs(x)))
)
shap_imp_2 <- data.frame(
  Variable=names(shap_2), Importance=apply(shap_2, MARGIN=2, FUN = function(x) sum(abs(x)))
)
```


```{r}
# Plot Shap-based variable importance
plot_shapley_1_bis <- ggplot(shap_imp_1, aes(reorder(Variable, Importance), Importance)) +
  aes(fill="darkred") + scale_fill_manual(values="darkred") +
  geom_col(fill="darkred") + xlab("Features") + ylab("Shapley value") + ylim(-0.1e-20, 0.2e-16) +
  ggtitle("") + theme_classic() + theme(legend.position="none", text=element_text(size=22)) + coord_flip()

plot_shapley_2_bis <- ggplot(shap_imp_2, aes(reorder(Variable, Importance), Importance)) +
  aes(fill="darkred") + scale_fill_manual(values="darkred") +
  geom_col(fill="darkred") + xlab("Features") + ylab("Shapley value") + ylim(-0.1e-20, 0.2e-16) +
  ggtitle("") + theme_classic() + theme(legend.position="none", text=element_text(size=22)) + coord_flip()

ggpubr::ggarrange(plot_shapley_1_bis, plot_shapley_2_bis) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Shapley values for the observations number 1 and 35 on the marks in Portugal dataset",
      size=22
      )
  )
```



# SHAP

For now, SHAP is not really well implemented in R as it is in Python with the shap module. I will show how to use 2 model agnostic packages: fastshap and shapper. The latest is the equivalent of the shap module in Python but it is still being implemented so there are not much results to extract from it.


## Fastshap package

In order to have a better understanding of the package fastshap, there are these two websites that are interesting: https://bgreenwell.github.io/fastshap/articles/fastshap.html and https://www.hfshr.xyz/posts/2020-06-07-variable-importance-with-fastshap/.

Here I took the German cars database. To calculate SHAP values, we need to code "adjust=TRUE" in the function but this is only experimental.

```{r}
library(fastshap)
```

```{r}
pred_fun <- function(model, newdata) {predict(model, newdata)}
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

shap <- fastshap::explain(
  rf_optimal_regression_bis, X=as.data.frame(train_regression), pred_wrapper=pred_fun, nsim=10,
  feature_names=c("mileage", "fuel", "gear", "offerType", "year"),
  newdata=as.data.frame(train_regression), adjust=TRUE ## this makes SHAP and not just Shapley values
  )

stopImplicitCluster()
```


### Feature importance plot

This plot shows the feature importance using the mean of the SHAP values. This is a substitute for permutation feature importance.

```{r}
autoplot(shap, type="importance") + 
  ggtitle("SHAP feature importance on the random forest on German cars dataset") +
  aes(fill="darkred") + scale_fill_manual(values="darkred") + xlab("Features") + 
  ylab("Mean of SHAP values") + ylim(0, 8000) + theme_classic() + 
  theme(legend.position="none", text=element_text(size=22))
```

The most important features seem to be gear, year and mileage.


### Dependence plot

This plot shows the dependence between the SHAP value and mileage. This is a substitute to ICE, ALE and PDP. I decided to use the mileage feature for the first plot.

```{r}
autoplot(shap, type="dependence", feature="mileage", X=train_regression, smooth=TRUE, color_by="gear") +
  ggtitle("Dependence plot of the feature mileage on the random forest on German cars dataset") +
  scale_color_manual(values=c("#1C9C07", "steelblue", "blue")) + xlim(0, 700000) +
  xlab("Mileage") + ylab("SHAP value") + theme_classic() + 
  theme(legend.position="bottom") + labs(color="Gear of the car") + theme(text=element_text(size=22))
```

```{r}
autoplot(shap, type="dependence", feature="gear", X=train_regression, smooth=FALSE) +
  geom_boxplot() + 
  ggtitle("Dependence plot of the feature gear on the random forest on German cars dataset") + 
  xlab("Gear of the car") + ylab("SHAP value") + theme_classic() + theme(text=element_text(size=22))
```


### Contribution plot

```{r}
SHAP_contrib_1 <- autoplot(shap, type="contribution", row_num=1) + aes(fill="darkred") +
  ggtitle("") + scale_fill_manual(values="darkred") + xlab("Feature") + ylab("SHAP value") +
  theme_classic() + theme(legend.position="none") + ylim(-75000, 50000) + theme(text=element_text(size=22))

SHAP_contrib_2 <- autoplot(shap, type="contribution", row_num=35) + aes(fill="darkred") +
  ggtitle("") + scale_fill_manual(values="darkred") + xlab("Feature") + ylab("SHAP value") +
  theme_classic() + theme(legend.position="none") + ylim(-75000, 50000) + theme(text=element_text(size=22))
```

```{r}
ggpubr::ggarrange(SHAP_contrib_1, SHAP_contrib_2) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Feature contributions for observation number 1 and 35 on the German cars dataset",
      size=22
      )
  )
```

For the marks in Portugal dataset, I could not calculate SHAP values.


## Shapper package

This package is the shap module from Python that is being implemented in R. For now, it is far from complete: https://github.com/ModelOriented/shapper and https://www.r-bloggers.com/2019/03/shapper-is-on-cran-its-an-r-wrapper-over-shap-explainer-for-black-box-models/.

## German cars dataset

```{r}
devtools::install_github("ModelOriented/shapper", force=TRUE)

library(shapper)
shapper::install_shap()
```

```{r}
pred_fun <- function(X.model, newdata) {predict(X.model, data=newdata)}
```

```{r}
ive_rf_regression_1 <- shapper::individual_variable_effect(
  rf_optimal_regression_bis, data=as.data.frame(train_regression), predict_function=pred_fun, 
  new_observation=as.data.frame(train_regression[1,]), nsamples=100
  )

ive_rf_regression_2 <- shapper::individual_variable_effect(
  rf_optimal_regression_bis, data=as.data.frame(train_regression), predict_function=pred_fun, 
  new_observation=as.data.frame(train_regression[35,]), nsamples=100
  )
```

```{r}
ive_plot_1 <- plot(ive_rf_regression_1) + ggtitle("") +
  ylab("SHAP value") + ylim(-50000, 60000) + theme(text=element_text(size=22))
ive_plot_2 <- plot(ive_rf_regression_2) + ggtitle("") +
  ylab("SHAP value") + ylim(-50000, 60000) + theme(text=element_text(size=22))

ggpubr::ggarrange(ive_plot_1, ive_plot_2) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Individual variable effect for observation number 1 and 35 on the German cars dataset",
      size=22
      )
  )
```


### Marks in Portugal dataset

```{r}
p_function <- function(model, data){predict(model, newdata=data, type="prob")}

ive_rf_classification_1 <- shapper::individual_variable_effect(
  rf_best_classification_bis, data=as.data.frame(train), predict_function=p_function, 
  new_observation=as.data.frame(train[1,]), nsamples=1000
  )

ive_rf_classification_2 <- shapper::individual_variable_effect(
  rf_best_classification_bis, data=as.data.frame(train), predict_function=p_function, 
  new_observation=as.data.frame(train[35,]), nsamples=1000
  )
```

```{r}
ive_plot_marks_1 <- plot(ive_rf_classification_1) + ggtitle("") + ylab("SHAP value") + 
  ylim(0, 1) + theme(text=element_text(size=22))
ive_plot_marks_2 <- plot(ive_rf_classification_2) + ggtitle("") + ylab("SHAP value") + 
  ylim(0, 1) + theme(text=element_text(size=22))

ggpubr::ggarrange(ive_plot_marks_1, ive_plot_marks_2) %>%
  ggpubr::annotate_figure(
    top=ggpubr::text_grob(
      "Individual variable effect for observation number 1 and 35 on the marks in Portugal dataset",
      size=22
      )
  )
```


