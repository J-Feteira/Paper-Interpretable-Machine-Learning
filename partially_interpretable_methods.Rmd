---
title: "Partially interpretable methods"
author: "Jérémy Feteira"
date: ""
lang: fr
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
  pdf_document:
    df_print: kable
    keep_tex: yes
    number_section: yes
    toc: yes
  html_document:
    df_print: paged
    code_folding: show
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10, sanitize = TRUE
  )
```

```{r package}
library(doParallel)
library(kableExtra)
library(readxl)
library(ggplot2)
```


```{r function}
# si above = TRUE on a un header_above
tab_fun <- function(tab, above = FALSE, title = title, font_size = 10, header = NULL){
  if(above){
    tab %>% kable(caption = title) %>%
    kable_styling(font_size = font_size, full_width=FALSE, stripe_color = "lightgray", stripe_index = 0,
                  bootstrap_options = c("striped"), position = "center") %>%
    add_header_above(header = header, bold=TRUE, color="red")%>%
    column_spec(1, bold=T) %>%
    row_spec(0, bold=T)
  } else {
    tab %>% kable(caption = title) %>%
      kable_styling(font_size = font_size, full_width=FALSE, stripe_color = "lightgray", stripe_index = 0,
                    bootstrap_options = c("striped"), position = "center") %>%
      column_spec(1, bold=T) %>%
      row_spec(0, bold=T)
  }
}
```



# Importation of the databases

```{r}
setwd("D:/Memoire/application")
#setwd("C:/Perso/Memoire/application")
```

```{r}
data_regression <- read_excel("data_regression.xlsx")
data_classification <- read_excel("data_classification.xlsx")
```

```{r}
table(is.na(data_classification))
table(is.na(data_regression))
```

# Random forest

```{r}
library(caret)
library(randomForest)
```

## On classification dataset

```{r}
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark [0,05]"] <-
  "mark [0,10]"
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark ]05,10]"] <-
  "mark [0,10]"
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark ]10,15]"] <-
  "mark [10,20]"
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark ]15,20]"] <-
  "mark [10,20]"

levels(as.factor(data_classification$average_mark_factor))
table(data_classification$average_mark_factor)
```

```{r}
library(rsample)
set.seed(3)

## split the dataframe in 2 with test and train data
data_classification_split <- data_classification %>% initial_split(prop = 0.8)

test <- data_classification_split %>% testing()
train <- data_classification_split %>% training()
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

rfGrid <- expand.grid(mtry = c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20))
ctrlCv <- trainControl(method = "repeatedcv", repeats = 100, number = 5)
rf.caret <- train(
  average_mark_factor ~ ., data = train, method = "rf", na.action = na.omit,
  trControl = ctrlCv, tuneGrid = rfGrid
  )

rf_optimal <- rf.caret$finalModel
#rf.caret$bestTune
stopImplicitCluster()

save(rf_optimal, file="best_model_random_forest_classification.RData")
```

## On regression dataset

I removed the feature hp because it was highly correlated with the feature price, which could lead to problems for certain methods.

```{r}
library(rsample)

#data_regression_bis <- data_regression[,-c(2,3)]
data_regression_bis <- data_regression[,-c(8)]
data_regression_bis$price <- as.integer(data_regression_bis$price)

set.seed(3)
## split the dataframe in 2 with test and train data
data_regression_split <- data_regression_bis %>% initial_split(prop = 0.75)

test_regression <- data_regression_split %>% testing()
train_regression <- data_regression_split %>% training()
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

rfGrid_regression <- expand.grid(mtry = c(2, 4, 10, 16))
ctrlCv_regression <- trainControl(method = "repeatedcv", repeats = 10, number = 5)
rf.caret_regression <- train(
  price ~ ., data = train_regression, method = "rf", na.action = na.omit,
  trControl = ctrlCv_regression, tuneGrid = rfGrid_regression
  )

rf_optimal_regression <- rf.caret_regression$finalModel
#rf.caret$bestTune
stopImplicitCluster()

save(rf_optimal_regression, file="best_model_random_forest_regression.RData")
```

# Load the 2 random forests

```{r}
load("D:/Memoire/application/best_model_random_forest_classification.RData")
load("D:/Memoire/application/best_model_random_forest_regression.RData")
```


# PDP

https://bgreenwell.github.io/pdp/articles/pdp.html

```{r}
rf <- randomForest(price ~ ., data = train_regression, method = "class", ntree = 50,
                   parms = list(split = "gini"), na.action=na.omit, keep.forest = TRUE,
                   importance = TRUE)
```


```{r}
library(pdp)
rf %>% partial(pred.var = "mileage", train=train_regression) %>%
  autoplot(smooth = TRUE, ylab = expression(f(mileage))) +
  theme_minimal() + ggtitle("ggplot2-based PDP")


```

It appears that PDP is not working with categorical feature.


# ICE

# ALE

# Feature interaction

# Permutation feature importance

```{r}
varImpPlot(rf_optimal)

library(vip)
vip(rf_optimal, bar=FALSE, horizontal=TRUE, size = 1.5) + 
  geom_col(width = 0.4, color = "darkred", fill = "darkred")

data.frame(vi=rf_optimal$importance, variable=rf_optimal$importance) %>%
  ggplot() + aes(x= reorder(variable,vi), y = vi) + 
  geom_col(width = 0.4, color = "darkred", fill = "darkred") +
  coord_flip() + labs(y = "Importance mesure", x = NULL) + theme_minimal()
```







