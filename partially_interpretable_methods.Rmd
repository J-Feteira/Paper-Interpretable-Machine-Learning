---
title: "Partially interpretable methods"
author: "Jérémy Feteira"
date: ""
lang: fr
output:
  rmdformats::readthedown:
    highlight: kate
    code_folding: show
  pdf_document:
    df_print: kable
    keep_tex: yes
    number_section: yes
    toc: yes
  html_document:
    df_print: paged
    code_folding: show
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10, sanitize = TRUE
  )
```

```{r package}
library(doParallel)
library(kableExtra)
library(readxl)
library(ggplot2)
```


```{r function}
# si above = TRUE on a un header_above
tab_fun <- function(tab, above=FALSE, title=title, font_size=10, header=NULL){
  if(above){
    tab %>% kable(caption=title) %>%
    kable_styling(
      font_size=font_size, full_width=FALSE, stripe_color="lightgray", stripe_index=0,
      latex_options=c("latex", "striped"), position="center"
      ) %>%
    add_header_above(header=header, bold=TRUE, color="red") %>%
    column_spec(1, bold=T) %>%
    row_spec(0, bold=T)
  } else {
    tab %>% kable(caption=title) %>%
      kable_styling(
        font_size=font_size, full_width=FALSE, stripe_color="lightgray", stripe_index=0,
        latex_options=c("latex", "striped"), position="center"
        ) %>%
      column_spec(1, bold=T) %>%
      row_spec(0, bold=T)
  }
}
```



# Importation of the databases

```{r}
setwd("D:/Memoire/application")
#setwd("C:/Perso/Memoire/application")
```

```{r}
data_regression <- read_excel("data_regression.xlsx")
data_classification <- read_excel("data_classification.xlsx")
```

```{r}
table(is.na(data_classification))
table(is.na(data_regression))
```

# Random forest

```{r}
library(caret)
library(randomForest)
```

## On classification dataset

```{r}
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark [0,05]"] <-
  "mark [0,10]"
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark ]05,10]"] <-
  "mark [0,10]"
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark ]10,15]"] <-
  "mark [10,20]"
data_classification$average_mark_factor[data_classification$average_mark_factor=="mark ]15,20]"] <-
  "mark [10,20]"

levels(as.factor(data_classification$average_mark_factor))
table(data_classification$average_mark_factor)
```

```{r}
require(rsample)
set.seed(3)

for(i in 1:32){
  i <- as.integer(i)
  data_classification[[i]] <- as.factor(data_classification[[i]])
}

## split the dataframe in 2 with test and train data
data_classification_split <- data_classification %>% initial_split(prop = 0.8)

test <- data_classification_split %>% testing()
train <- data_classification_split %>% training()
```

```{r}
load("D:/Memoire/application/best_model_random_forest_classification.RData")
```


## On regression dataset

I removed the feature hp because it was highly correlated with the feature price, which could lead to problems for certain methods.

```{r}
require(rsample)

data_regression_bis <- data_regression[,-c(8)]
data_regression_bis$price <- as.integer(data_regression_bis$price)

for(i in c(2:6, 8)){
  i <- as.integer(i)
  data_regression_bis[[i]] <- as.factor(data_regression_bis[[i]])
}

set.seed(3)
## split the dataframe in 2 with test and train data
data_regression_split <- data_regression_bis[,-c(2,3)] %>% initial_split(prop = 0.75)

test_regression <- data_regression_split %>% testing()
train_regression <- data_regression_split %>% training()
```

```{r}
load("D:/Memoire/application/best_model_random_forest_regression.RData")
```

# Random forests used

## German cars dataset

```{r}
## problem with prediction when doing partial function so I trained again the model with the 
## best parameters of the cross validation
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

rf_optimal_regression_bis <- randomForest(price ~ .,
  data=train_regression, method = "class", ntree = 500,
  parms = list(split = "gini"), mtry = 7, na.action = na.omit
)

stopImplicitCluster()

save(rf_optimal_regression_bis, file="rf_model_regression_PDP.RData")
```

```{r}
load("D:/Memoire/application/rf_model_regression_PDP.RData")
```

## Marks in Portugal dataset

```{r}
## problem with prediction when doing partial function so I trained again the model with the 
## best parameters of the cross validation
rf_best_classification_bis <- randomForest(average_mark_factor ~ .,
  data=train, method = "class", ntree = 500,
  parms = list(split = "gini"), mtry = 16, na.action = na.omit
)
```


# PDP

In order to have a bettter understanding on how to use PDPs, I took inspiration from this website: https://bgreenwell.github.io/pdp/articles/pdp.html. I decided to do the PDP on a random forest using the German cars dataset for the numeric features example and the marks in Portugal for the example with the categorical features.

The first things we need to do is to look at the distribution of the features in order to make no mistakes and not overinterpret some results.

```{r}
## mileage
ggplot(data=data_regression_bis) + geom_histogram(fill="darkred", col="grey", binwidth=5000) + 
  aes(mileage) + labs(y="Number", x="Mileage of the cars") + 
  theme_classic() + ggtitle("Histogram of the mileage of the cars")
```

```{r}
## price
ggplot(data=data_regression_bis) + geom_histogram(fill="darkred", col="grey", binwidth=5000) + 
  aes(price) + labs(y="Number", x="Price of the cars") + 
  theme_classic() + ggtitle("Histogram of the price of the cars")
```

With a numerical feature, we get the following result:

```{r}
require(pdp)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

rf_optimal_regression_bis %>%
  partial(pred.var="mileage", train=train_regression, rug=TRUE) %>%
  autoplot(smooth=TRUE) +
  theme_classic() + ggtitle("PDP of the random forest on the German cars dataset") +
  ylab("Predicted price") + xlab("Mileage")

stopImplicitCluster()
```

From what we can see, it looks like that the average predicted price is decreasing when mileage increase. It is also important to note that we cannot interpret when the mileage is strictly above 200 000 because there are not much data.

With a categorical feature, we get the following result.

The first plot is from the randomForest package. This is the plot we get when doing a random forest:

```{r}
partialPlot(
  x=rf_best_classification_bis, pred.data=as.data.frame(test), x.var=failures, 
  which.class="mark [0,10]", ylab="Predicted average mark probability",
  xlab="Failures", las=1, main="PDP of the random forest on the marks in Portugal dataset",
  ylim=c(-0.6, 0.6)
  )
```

This plot shows that students that have 1 or more failures have a higher probability of having a mark between 0 and 10.

The second plot is a function that can be used for every model:

```{r}
rf_best_classification_bis %>%
  partial(pred.var = "failures", train=train) %>%
  autoplot(smooth=FALSE) +
  theme_classic() + ggtitle("PDP of the random forest on the marks in Portugal dataset") +
  ylab("Predicted average mark probability") + xlab("Failures") + ylim(-0.8, 0.6)
```

As the previous plot, this plot shows that the probability of having a mark between 0 and 10 is higher when a student has at least once failure.

These interpretation were made because I already worked on the data and did some factorial analysis previously but this type of plot needs to be carefully interpreted. Moreover, PDP does not work if the categorical features used in the model are not factors so a conversion into factor for each feature is needed before doing the model.


# ICE

To make an ICE plot, we need to download the package ICEbox. In order to create my own plot, I looked at those two websites: https://arxiv.org/pdf/1309.6392.pdf and  https://rdrr.io/cran/ICEbox/man/plot.ice.html.

I decided to use the dataframe on the German car dataset and my random forest used for the PDP with the price as outcome and the mileage as predictor. The first thing to do is to built the ICE.

```{r}
require(ICEbox)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

ICE_model <- ice(
  object=rf_optimal_regression_bis, X=as.data.frame(train_regression),
  y=train_regression$price, predictor="mileage", frac_to_build = .1
  )

stopImplicitCluster()

save(ICE_model, file="ICE_model.RData")
```

Then we can plot the curves, with the PDP curve in yellow and the individuals in grey. I decided to use the quantiles because the mileage has a lot of low values but not much high values.

```{r}
load("D:/Memoire/application/ICE_model.RData")
```

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

plot(
  ICE_model, x_quantile=TRUE, plot_pdp=TRUE, frac_to_plot=0.75, las=1, xlab="Quantile of mileage",
  ylab="Predicted price", main="ICE plot on the German car dataset", cex.axis=0.75, 
  plot_orig_pts_preds=FALSE
  )

stopImplicitCluster()
```

For most cars there is a decrease in predicted price when mileage increases until the first decile. Then between the first and second decile, there is an increase in predicted prices before it drops again. The explanation behind this increase is the fact that some expensive cars are certainly represented at this quantile. Moreover, it looks like the predicted price does not decrease much when the mileage has reached the third decile, which means with an average of `r quantile(train_regression$mileage,0.3)`kms.

Finally, we can note that ICE does not work with categorical features.


# ALE

In order ot create ALE plot, I took inspiration from these two websites: http://xai-tools.drwhy.ai/ALEplot.html and  https://cran.r-project.org/web/packages/ALEPlot/ALEPlot.pdf.

I used the German cars dataset to do my plots. One plot is with the feature mileage.

```{r}
require(ALEPlot)

pred_fun <- function(X.model, newdata) {
  predict(X.model, newdata)
  }
```

```{r}
ALE_regression <- ALEPlot(
  X=as.data.frame(train_regression), X.model=rf_optimal_regression_bis, J=c(1),
  pred.fun=pred_fun
  )
```

The complete plot is the following:

```{r}
ggplot() + geom_line(col="darkred", stat="identity") + 
  aes(x=ALE_regression$x.values, y=ALE_regression$f.values, xend=max(ALE_regression$x.values),
      yend=max(ALE_regression$f.values)) + 
  labs(y="ALE of price", x="Mileage") + theme_classic() +
  ggtitle("ALE plot of the German cars dataset")
```

Because there is only one row that is beyond 250000, if we remove this value, we get:

```{r}
ggplot() + geom_line(col="darkred", stat="identity") + 
  aes(x=ALE_regression$x.values, y=ALE_regression$f.values, xend=max(ALE_regression$x.values),
      yend=max(ALE_regression$f.values)) + 
  labs(y="ALE of price", x="Mileage") + theme_classic() +
  ggtitle("ALE plot of the German cars dataset") + xlim(0, 220000)
```

This plot shows that the average prediction of prices decreases when mileage increase. We can note that the pic around 40000 is certainly due to the fact that there are more expensive cars that were sold.

The other plot is with the feature gear, to show how it is presented with a categorical value:

```{r}
ALE_regression_gear <- ALEPlot(
  X=as.data.frame(train_regression), X.model=rf_optimal_regression_bis, J=c("gear"),
  pred.fun=pred_fun, NA.plot=FALSE
  )
```

```{r}
ggplot() + geom_bar(fill="darkred", col="grey", stat="identity") + 
  aes(x=ALE_regression_gear$x.values, y=ALE_regression_gear$f.values) + 
  labs(y="ALE of price", x="Gear") + theme_classic() +
  ggtitle("ALE plot of the German cars dataset") + ylim(-5000, 10000)
```

This plot shows that it looks like that the predicted price increases when the car has automatic gear.


# Feature interaction

For this part, I decided to use my RuleFit model used in the first part.

```{r}
load("C:/Perso/Memoire/application/best_model_RuleFit.RData")
```

```{r}
for(i in c(2:6, 9)){
  i <- as.integer(i)
  data_regression[[i]] <- as.factor(data_regression[[i]])
}
```


```{r}
library(pre)

cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

RuleFit_model <- pre(
  price~gear+mileage, data=data_regression,
  par.final=TRUE ## parallelization for selecting final ensemble
  )

stopImplicitCluster()
```

```{r}
save(RuleFit_model, file="RuleFit_model.RData")
load("C:/Perso/Memoire/application/RuleFit_model.RData")
```


To look at the presence of interactions between the explanatory features:

```{r}
cl <- detectCores() %>% -1 %>% makeCluster
registerDoParallel(cl)

set.seed(77)
null_interaction <- pre::bsnullinteract(RuleFit_model, nsamp = 10, parallel=TRUE)
int <- pre::interact(RuleFit_model, nullmods=null_interaction)

stopCluster(cl)
```



```{r}
save(null_interaction, file="null_interaction.RData")
load("C:/Perso/Memoire/application/null_interaction.RData")
```





# Permutation feature importance

We can also look at the feature importance in order to have a better understanding at our model. For this part, I used my two random forests created previously on the German cars dataset and the marks in Portugal dataset. I decided to show how to use the vip function of the vip package because it is model-agnostic.

```{r}
library(vip)
vip(rf_optimal_regression_bis, bar=TRUE, horizontal=TRUE, aesthetics=list(fill="darkred")) +
  theme_classic() + geom_col(fill="darkred") + xlab("Feature") + ylab("Importance") + 
  ggtitle("Feature importance for German cars dataset")
```

It looks like that the most important feature is the mileage feature in our random forest.

It is also working with categorical features. With the random forest on the marks in Portugal dataset, we get the following plot:

```{r}
vip(rf_best_classification_bis, bar=TRUE, horizontal=TRUE, aesthetics=list(fill="darkred")) +
  theme_classic() + geom_col(fill="darkred") + xlab("Feature") + ylab("Importance") + 
  ggtitle("Feature importance for the marks in Portugal dataset")
```

The most important feature of the random forest is failures.
